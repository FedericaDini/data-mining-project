{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xclara: C = 3, |D| = 3000, F = 2\n",
    "# s-set1: C = 15, |D| = 5000, F = 2\n",
    "# pendigits: C = 10, |D| = 10992, F = 16\n",
    "# waveform: C = 3, |D| = 5000, F = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "ds = 'xclara' #'xclara','s-set1','pendigits','waveform'\n",
    "print(ds)\n",
    "df = pd.read_csv(f'dataset/{ds}.csv')\n",
    "display(df.head(2))\n",
    "print('n_cluster',len(df.iloc[:,-1].value_counts()))\n",
    "print('n_feature',df.shape[1]-1)\n",
    "N_clusters = len(df.iloc[:,-1].value_counts())\n",
    "N_features = df.shape[1]-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Inserisco i vari parametri**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_clusters: int = 3\n",
    "# N_features:int = 2\n",
    "random_state = centers_seed = random_clients_seed = iid_seed = shuffle_seed = 2 #123\n",
    "factor_lambda:float = 2\n",
    "epsilon:float = 0.005\n",
    "max_number_rounds:int = 30\n",
    "mode = 'iid' # iid  |  non_iid_volume  |  non_iid_distr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Definisco le funzioni per le nrme e inizializzo i centroidi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import math\n",
    "from random import randint\n",
    "from numba import jit\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score as ARI\n",
    "FROBENIUS_NORM = 'fro'\n",
    "\n",
    "\n",
    "#@jit(nopython=True)\n",
    "#def numba_norm(u:np.ndarray, v:np.ndarray):\n",
    "#    return np.linalg.norm(u - v)\n",
    "\n",
    "def norm_fro(u:np.ndarray):\n",
    "    return np.linalg.norm(u, ord = FROBENIUS_NORM)\n",
    "\n",
    "# hungarian method (https://smorbieu.gitlab.io/accuracy-from-classification-to-clustering-evaluation/)\n",
    "def perm_norm_fro(u:np.ndarray, v:np.ndarray):\n",
    "    cm = cdist(u, v)\n",
    "#     print(cm)\n",
    "    indexes = linear_assignment(cm)\n",
    "    indexes = np.transpose(indexes)\n",
    "    js = [e[1] for e in sorted(indexes, key=lambda x: x[0])]\n",
    "#     print(js)\n",
    "    v = np.asarray(v)\n",
    "    original_norm = np.linalg.norm(u-v, ord = FROBENIUS_NORM)\n",
    "    permuted_norm = np.linalg.norm(u-v[js], ord = FROBENIUS_NORM)\n",
    "#     print('original centers: ',v)\n",
    "#     print('permuted centers: ',v[js])\n",
    "#     print('original norm: ',original_norm)\n",
    "#     print('permuted norm: ',permuted_norm)\n",
    "    print()\n",
    "    return permuted_norm\n",
    "\n",
    "\n",
    "def generate_random_centers(seed: int, n_clusters: int, n_features: int):\n",
    "    import numpy as np\n",
    "    np.random.seed(seed)\n",
    "    centers = np.random.rand(n_clusters,n_features)\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FederatedFCMeans Client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCMeansFederatedClient:\n",
    "\n",
    "    def initialize(self, params: Dict) -> None:\n",
    "        self.__dataset = params['dataset']\n",
    "        self.__num_features = len(self.__dataset[0])\n",
    "        self.__classes = [-1] * len(self.__dataset)\n",
    "        self.__distance_fn = params['distance_fn']\n",
    "        self.__factor_lambda = params['factor_lambda']\n",
    "        self.y_true = params['y_true']\n",
    "        \n",
    "    def evaluate_cluster_assignment(self, centers: List) -> List[Tuple]:\n",
    "        return self.__local_sums(centers)\n",
    "\n",
    "    def finalize(self, centers: List[np.array]) -> None:\n",
    "        \n",
    "\n",
    "    def __local_sums(self, centers: List[np.array]):\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FederatedFCMeans Server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCMeansFederatedServer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__current_round = 0\n",
    "\n",
    "    def initialize(self, params: Dict) -> None:\n",
    "        self.__epsilon = params['epsilon']\n",
    "        self.__max_number_rounds = params.get('max_number_rounds', 10)\n",
    "        self.__num_clusters = params['num_clusters']\n",
    "        self.__num_features = params['num_features']\n",
    "        self.__norm_fm = params['norm_fn']\n",
    "        self.__fnorms = []\n",
    "        self.__cluster_centers = []\n",
    "        self.__cluster_centers.append(params['centers'])\n",
    "\n",
    "    def next_round(self) -> bool:\n",
    "        num_clusters = self.__num_clusters\n",
    "        cluster_centers = self.__cluster_centers\n",
    "        \n",
    "        num_centers = len(cluster_centers)\n",
    "        if (num_centers > 1):\n",
    "            centers_r = np.array(cluster_centers[-1])\n",
    "            centers_r_1 = np.array(cluster_centers[-2])\n",
    "            tot_diff_sum = self.__norm_fm(centers_r- centers_r_1)\n",
    "            self.__fnorms.append(tot_diff_sum)\n",
    "#             if (tot_diff_sum < self.__epsilon):\n",
    "#                 return False\n",
    "        \n",
    "        result = self.__current_round < self.__max_number_rounds\n",
    "        \n",
    "        self.__current_round = self.__current_round + 1 if result else 0\n",
    "        return result\n",
    "\n",
    "    def process_clustering_results(self, client_responses: List):\n",
    "        \"\"\"\n",
    "        num_clients = len(client_responses)\n",
    "        num_clusters = self.__num_clusters\n",
    "\n",
    "        #client_responses is a list of list of tuples where the first element of this tuple is u and second element is ws\n",
    "        num_features = self.__num_features\n",
    "        u_list = [0] * num_clusters\n",
    "        ws_list = [[0] * num_features for i in range(num_clusters)]\n",
    "        \n",
    "        for client_idx in range(num_clients):\n",
    "            # remember the response is a list of tuples where each tuple represents the (u, ws) for each cluster\n",
    "            response = client_responses[client_idx]\n",
    "            for i in range(num_clusters):\n",
    "                client_u = response[0][i]\n",
    "                client_ws = response[1][i] if response[1][i] is np.array else np.array(response[1][i])\n",
    "                u_list[i] = u_list[i] + client_u\n",
    "                ws_list[i] = ws_list[i] + client_ws\n",
    "\n",
    "        new_cluster_centers = []\n",
    "        prev_cluster_centers = self.__cluster_centers[-1]\n",
    "        \n",
    "        for i in range(num_clusters):\n",
    "            u = u_list[i]\n",
    "            ws = ws_list[i]\n",
    "            if (u == 0):\n",
    "                center = prev_cluster_centers[i]\n",
    "            else:\n",
    "                center = ws/u\n",
    "            new_cluster_centers.append(np.array(center))\n",
    "\n",
    "        self.__cluster_centers.append(new_cluster_centers)\n",
    "        \"\"\"\n",
    "        \n",
    "    def get_centers(self) -> List:\n",
    "        return self.__cluster_centers[-1]\n",
    "\n",
    "    #@property\n",
    "    def current_round(self) -> int:\n",
    "        return self.__current_round\n",
    "\n",
    "    def finalize(self, enabled_print: bool = False) -> None:\n",
    "        \"\"\" HERE WE CAN SAVE FOR EACH ROUND THE VALUES OF THE CENTERS \"\"\"\n",
    "        centers = self.__cluster_centers[1:]\n",
    "        fnorms = self.__fnorms\n",
    "        \n",
    "        \n",
    "        if (enabled_print):\n",
    "            if N_features ==2:\n",
    "                for i in range(len(centers)):\n",
    "                    plt.figure()\n",
    "                    plt.title(f\"round {i}, Frobenius norm {fnorms[i]:.15f}\")\n",
    "                    plt.scatter(X[:,0],X[:,1])\n",
    "                    plt.scatter(np.asarray(centers[i])[:,0],np.asarray(centers[i])[:,1],s = 30)\n",
    "        return (fnorms, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Genero i chunks del dataset (prima bilanciati e poi, eventualmente, sbilanciati)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle as shuffleSK\n",
    "\n",
    "#UPDATE: ADDED iid_seed and shuffle_seed parameters\n",
    "\n",
    "def generate_dataset_chunks(X: np.array, \n",
    "                            Y: np.array, \n",
    "                            n_splits: int, \n",
    "                            shuffle: bool = False, \n",
    "                            shuffle_seed:int = None, \n",
    "                            mode: str = 'iid', \n",
    "                            iid_seed: int = None):\n",
    "    if (n_splits == 1):\n",
    "        return [X],[Y],[]\n",
    "    dataset_chunks = []\n",
    "    y_chunks = []\n",
    "    all_indices = []\n",
    "\n",
    "    if mode == 'iid':\n",
    "        skf = StratifiedKFold(n_splits = n_splits, shuffle = shuffle, random_state = iid_seed)\n",
    "        for train_index, test_index in skf.split(X, Y):\n",
    "            dataset_chunks.append(X[test_index])\n",
    "            y_chunks.append(Y[test_index])\n",
    "            all_indices.extend(test_index)\n",
    "\n",
    "    \"\"\"\n",
    "    elif mode == 'non_iid_volume':\n",
    "        X, Y = shuffleSK(X,Y, random_state = shuffle_seed)\n",
    "\n",
    "        factor = 1.2\n",
    "        total = len(X)-int(np.floor(len(X)/factor))\n",
    "        print(total)\n",
    "        temp = []\n",
    "        for i in range(n_splits-1):\n",
    "            val = np.random.randint(0, total)\n",
    "            temp.append(val)\n",
    "            total -= val\n",
    "        temp.append(total)\n",
    "        nums = [z+int(np.floor(len(X)/factor/n_splits)) for z in temp]\n",
    "        cumsum = list(np.cumsum(nums))\n",
    "        old = [x for x in cumsum]\n",
    "        cumsum.pop(-1)\n",
    "        cumsum.insert(0,0)\n",
    "        for s,e in zip(cumsum,old):\n",
    "            dataset_chunks.append(X[s:e])\n",
    "            # not tested\n",
    "            y_chunks.append(Y[s:e])\n",
    "    elif mode == 'non_iid_distr':\n",
    "        dataset_chunks = [X[i:i + int(np.floor(len(X)/n_splits)),:] for i in range(0, len(X), int(np.floor(len(X)/n_splits)))]\n",
    "        # not tested\n",
    "        y_chunks.append([Y[i:i + int(np.floor(len(X)/n_splits)),:] for i in range(0, len(X), int(np.floor(len(X)/n_splits)))])\n",
    "    return dataset_chunks,y_chunks, all_indices\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original = df.iloc[:,:-1].values\n",
    "Y = df.iloc[:,-1].values\n",
    "df.iloc[:,-1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plot dei cluster (con 2 features)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N_features == 2:\n",
    "    plt.scatter(X_original[:,0], X_original[:,1],c = Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plot dei cluster normalizzato (con 2 features)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X_original)\n",
    "# X = X_original\n",
    "if N_features == 2:\n",
    "    plt.scatter(X[:,0],X[:,1], c = Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Execution Logic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(N_clients: int, \n",
    "                   N_min_clients: int, \n",
    "                   centers: List[np.array], \n",
    "                   dataset_chunks: List[np.array], \n",
    "                   y_chunks: List, \n",
    "                   client_parms: Dict[str, bytes],\n",
    "                   server_params: Dict[str, bytes], \n",
    "                   enabled_print: bool = False, \n",
    "                   random_clients: bool = True,\n",
    "                   random_clients_seed: int = None):\n",
    "    #UPDATE: SEED INITIALIZATION MOVED HERE\n",
    "    import random\n",
    "    random.seed(random_clients_seed)\n",
    "\n",
    "    # (1) CLIENTS INITIALIZATION ==================================================================\n",
    "    clients = []\n",
    "\n",
    "    for cli in range(N_clients):\n",
    "        \n",
    "        client_dataset = dataset_chunks[cli]\n",
    "        params = dict(client_parms)\n",
    "        client_y = y_chunks[cli]\n",
    "        params['dataset'] = client_dataset\n",
    "        params['y_true'] = client_y\n",
    "\n",
    "        client = FCMeansFederatedClient()\n",
    "        client.initialize(params)\n",
    "        clients.append(client)\n",
    "\n",
    "    print(f'starting server, total number of clients {N_clients}, min number of clients {N_min_clients}')\n",
    "\n",
    "    active_clients_idx: List = None\n",
    "    all_clients = (N_clients == N_min_clients)\n",
    "\n",
    "    if (not random_clients and not all_clients):\n",
    "        active_clients_idx = random.sample(range(N_clients), N_min_clients)\n",
    "\n",
    "\n",
    "    # (2) SERVER INITIALIZATION ==================================================================\n",
    "    server = FCMeansFederatedServer()\n",
    "    server.initialize(server_params)\n",
    "\n",
    "    # (3) FEDERATED TRAINING ====================================================================\n",
    "    while (server.next_round()):\n",
    "\n",
    "        if (random_clients and not all_clients):\n",
    "            active_clients_idx = random.sample(range(N_clients), N_min_clients)\n",
    "        \n",
    "        centers = server.get_centers()\n",
    "        client_responses = []\n",
    "        client_responses_append = client_responses.append\n",
    "        for cli, client in enumerate(clients):\n",
    "            if (not all_clients and cli not in active_clients_idx):\n",
    "                continue\n",
    "            response = client.evaluate_cluster_assignment(centers)\n",
    "            client_responses_append(response)\n",
    "\n",
    "        server.process_clustering_results(client_responses)\n",
    "    \n",
    "    centers = server.get_centers()\n",
    "    # (4) EXECUTING SOME FINAL LOGIC ==================================================================\n",
    "    ari_list = []\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "    for client in clients:\n",
    "        u_matrix,y_pred = client.finalize(centers)\n",
    "        y_true = client.y_true\n",
    "        ari_list.append(ARI(y_true,y_pred))\n",
    "        y_pred_list.extend(y_pred)\n",
    "        y_true_list.extend(y_true)\n",
    "#     print(u_matrix,y_pred)\n",
    "    ari_mean = np.mean(np.asarray(ari_list))\n",
    "    ari_global = ARI(y_true_list,y_pred_list)\n",
    "\n",
    "    print('\\tmedia: ',ari_mean)\n",
    "    print('\\tglobal:',ari_global)\n",
    "\n",
    "    return server.finalize(enabled_print), ari_mean,ari_global, y_pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plots Definitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plot che mostra dopo quante iterazioni raggiungiamo la convergenza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_fnorm_by_round_experiment_plot(all_forms_list, epsilon, dataset_file,N_min_clients_percents,rnd = True):\n",
    "    fig, ax = plt.subplots(figsize = (9,4))\n",
    "\n",
    "    marker = itertools.cycle(('d', 's', 'o', '*','<','>','+','x')) \n",
    "    labels = [f'{label:.2f}' for label in N_min_clients_percents]\n",
    "    \n",
    "    \n",
    "    \n",
    "    val = np.nanmean(all_forms_list,axis = 0)\n",
    "    std = np.nanstd(all_forms_list,axis = 0)\n",
    "    for i in range(len(val)):\n",
    "        Y_values = val[i]\n",
    "        X_labels = [i for i in range(1, len(Y_values) + 1)]\n",
    "        plt.plot(X_labels, Y_values, label = labels[i], marker = next(marker),linestyle = '--')\n",
    "        Y_std = std[i]\n",
    "        plt.fill_between(X_labels, Y_values-Y_std, Y_values+Y_std, alpha = 0.2)\n",
    "\n",
    "    \n",
    "#     plt.ylim([-0.01,1])\n",
    "    plt.ylim([-0.01,0.1])\n",
    "\n",
    "    #threshold line\n",
    "    X_axis_threshold = [i for i in range(1, max_number_rounds + 1)]\n",
    "    Y_axis_threshold = [epsilon] * max_number_rounds\n",
    "    plt.text(1,epsilon+0.003,r'$\\varepsilon$',size = 14)\n",
    "    plt.plot(X_axis_threshold, Y_axis_threshold , 'k--', alpha=0.9)\n",
    "    plt.xticks(range(1,max_number_rounds+1,2))\n",
    "    plt.grid()\n",
    "\n",
    "    # Labeling the X-axis \n",
    "    plt.xlabel('t (round)',size = 14) \n",
    "    plt.ylabel(r'$||V^{(t+1)} - V^{(t)}||_F$',size = 14) \n",
    "    legend = plt.legend(title=r\"$\\gamma$\",fontsize = 14)\n",
    "    plt.setp(legend.get_title(),fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/'+ds+'_round'+'_random'*rnd+'.pdf',format='pdf',bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plot che mostra la proporzione dei partecipanti rispetto all'errore (0.25, 0.50, 0.75, 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_fnorm_by_experiment_plot(N_min_clients_percents_param, fro_list_param,rnd):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "\n",
    "    X_labels = [f'{per:.2f}' for per in N_min_clients_percents_param]\n",
    "    Y_values = fro_list_param.mean(axis = 0)\n",
    "    Y_std = fro_list_param.std(axis = 0)\n",
    "    \n",
    "    plt.plot(X_labels, Y_values,'ko-')\n",
    "    plt.fill_between(X_labels, Y_values-Y_std, Y_values+Y_std, color = 'k',alpha = 0.1)\n",
    "    plt.grid()\n",
    "#     plt.ylim([-0.0005,0.016])\n",
    "\n",
    "    # Labeling the X-axis \n",
    "    plt.xlabel(r\"Fraction of participants $\\gamma$\", size = 14) \n",
    "    plt.ylabel(r'$||V_{fed}(\\gamma) - V_{sum}||_F$',size = 14) \n",
    "#     plt.ylabel(r'$Fro(V_{federated} - V_{centralized})$') \n",
    "#     plt.title(f'Variation of Frobenius norm value for each % participant experiment. Dataset: {dataset_file}.',fontweight=\"bold\") \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/'+ds+'_centr'+'_random'*rnd+'.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Esecuzione dell'esperimento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Definisco la funzione per eseguire l'algoritmo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISTRIBUITED APPROACH\n",
    "def df_complete_experiment(random_clients:bool = False, \n",
    "                           centers_seed: int = None,\n",
    "                           random_clients_seed: int = None, \n",
    "                           num_splits:int = 10):\n",
    "    centers = generate_random_centers(centers_seed, N_clusters, N_features)\n",
    "\n",
    "    N_clients:int = 20\n",
    "    shuffle_dataset:bool = True\n",
    "    dataset_chunks,y_chunks,all_indices = generate_dataset_chunks(X, Y, N_clients, shuffle = shuffle_dataset, mode = mode, shuffle_seed = shuffle_seed, iid_seed = iid_seed)\n",
    "\n",
    "    enabled_print:bool = False\n",
    "\n",
    "    client_params: Dict[str, bytes] = {\n",
    "        'distance_fn': numba_norm,\n",
    "        'factor_lambda': factor_lambda\n",
    "    }\n",
    "\n",
    "    server_params: Dict[str, bytes] = {\n",
    "        'epsilon': epsilon,\n",
    "        'max_number_rounds': max_number_rounds,\n",
    "        'num_clusters': N_clusters,\n",
    "        'num_features': N_features,\n",
    "        'norm_fn': norm_fro,\n",
    "        'centers': centers\n",
    "    }\n",
    "\n",
    "    ### DISTRIBUITED APPROACH, GOES FROM 0.25 to 1.0 percentage of participants from a total of N_clients available participants\n",
    "    N_min_clients_percents: List[float] = [float(1/num_splits* i)  for i in range(1, num_splits + 1)]\n",
    "    print(N_min_clients_percents)\n",
    "    centers_list : List[List] = []\n",
    "    all_forms_list: List[List] = []\n",
    "    ari_clients_list: List[List] = []\n",
    "    ari_clients_global: List[List] = []\n",
    "    all_y_pred_fed: List[List] = []\n",
    "    all_ari_fed_sum: List[List] = []\n",
    "    for i in range(num_splits):\n",
    "        per_clients = N_min_clients_percents[i]\n",
    "        N_min_clients:int = math.floor(N_clients * per_clients)\n",
    "        (fnorms, centers),ari_clients_mean,ari_global,y_pred_fed = run_experiment(N_clients, N_min_clients, \n",
    "                                                                                  centers, dataset_chunks, \n",
    "                                                                                  y_chunks, client_params, \n",
    "                                                                                  server_params, enabled_print,\n",
    "                                                                                  random_clients, \n",
    "                                                                                  random_clients_seed = random_clients_seed)\n",
    "        ari_clients_list.append(ari_clients_mean)\n",
    "        ari_clients_global.append(ari_global)\n",
    "        all_forms_list.append(fnorms)\n",
    "        all_y_pred_fed.append(np.asarray(y_pred_fed))\n",
    "        centers_list.append(centers[-1])\n",
    "\n",
    "    ### CENTRALIZED APPROACH\n",
    "    N_clients:int = 1\n",
    "    shuffle_dataset:bool = True\n",
    "    dataset_chunks,y_chunks,_ = generate_dataset_chunks(X, Y, N_clients, shuffle = shuffle_dataset)\n",
    "    N_min_clients:int = math.floor(N_clients * per_clients)\n",
    "    (fnorms_centralized, centers_centralized),_,_,y_pred_sum = run_experiment(N_clients, N_min_clients, centers, dataset_chunks,y_chunks, client_params, server_params, enabled_print,\n",
    "                                                             random_clients, random_clients_seed = random_clients_seed)\n",
    "\n",
    "    centralized_app_center = np.array(centers_centralized[-1])\n",
    "    for gamma_pred_fed in all_y_pred_fed:\n",
    "        all_ari_fed_sum.append(ARI(gamma_pred_fed,np.asarray(y_pred_sum)[all_indices]))\n",
    "    return (centers_list, \n",
    "            all_forms_list, \n",
    "            centralized_app_center, \n",
    "            fnorms_centralized,\n",
    "            N_min_clients_percents,\n",
    "            ari_clients_list,\n",
    "            ari_clients_global,\n",
    "            all_ari_fed_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 4\n",
    "random_clients = True\n",
    "\n",
    "fro_list_all = []\n",
    "all_forms_list_all = []\n",
    "ari_clients_list_all = []\n",
    "ari_clients_global_all = []\n",
    "ari_pred_fed_sum_all = []\n",
    "for rep in range(10):\n",
    "    print(rep,'seed')\n",
    "    centers_list, all_forms_list, centralized_app_center, fnorms_centralized, N_min_clients_percents,ari_clients_list,ari_clients_global,all_ari_fed_sum = df_complete_experiment(\n",
    "        random_clients,\n",
    "        rep, # vario i centri \n",
    "        rep, # vario i client che seleziono\n",
    "        num_splits)\n",
    "    \n",
    "    fro_list: List[float] = []\n",
    "    for i in range(num_splits):\n",
    "        center_i = centers_list[i]\n",
    "        #fro_list.append(norm_fro(centralized_app_center - center_i)) # originale\n",
    "        fro_list.append(perm_norm_fro(centralized_app_center, center_i))\n",
    "    fro_list_all.append(fro_list)\n",
    "    for l in all_forms_list:\n",
    "        l += [np.nan] * (max_number_rounds - len(l))\n",
    "    all_forms_list_all.append(all_forms_list)\n",
    "    ari_clients_list_all.append(ari_clients_list)\n",
    "    ari_clients_global_all.append(ari_clients_global)\n",
    "    ari_pred_fed_sum_all.append(all_ari_fed_sum)\n",
    "    print()\n",
    "fro = np.asarray(fro_list_all)\n",
    "all_fnorms = np.asarray(all_forms_list_all)\n",
    "ari_clients = np.asarray(ari_clients_list_all)\n",
    "ari_global = np.asarray(ari_clients_global_all)\n",
    "ari_fed_sum = np.asarray(ari_pred_fed_sum_all)\n",
    "ari_fed_sum.shape, ari_global.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plots dei risultati**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tabella ARI a seconda della percentuale dei partecipanti**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame()\n",
    "metrics['ari_global_mean'] = ari_global.mean(axis = 0)\n",
    "metrics['ari_global_std'] = ari_global.std(axis = 0)\n",
    "metrics['ari_clients_mean'] = ari_clients.mean(axis = 0)\n",
    "metrics['ari_clients_std'] = ari_clients.std(axis = 0)\n",
    "metrics['fro_centers_mean'] = fro.mean(axis = 0)\n",
    "metrics['fro_centers_std'] = fro.std(axis = 0)\n",
    "metrics['ari_pred_fed_mean'] = ari_fed_sum.mean(axis = 0)\n",
    "metrics['ari_pred_fed_std'] = ari_fed_sum.std(axis = 0)\n",
    "\n",
    "metrics = metrics.round(5)\n",
    "metrics.index = [f'{label:.2f}' for label in N_min_clients_percents]\n",
    "metrics\n",
    "metrics.to_csv(f'results/{ds}_metrics.csv')\n",
    "pd.read_csv(f'results/{ds}_metrics.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plot che mostra la proporzione dei partecipanti rispetto all'errore (0.25, 0.50, 0.75, 1) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_fnorm_by_experiment_plot(N_min_clients_percents, fro,rnd = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plot che mostra dopo quante iterazioni raggiungiamo la convergenza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_fnorm_by_round_experiment_plot(all_fnorms, epsilon, ds, N_min_clients_percents,rnd = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
